{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "102c9698",
   "metadata": {},
   "source": [
    "# Import Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c8c600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import libraries and setup\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, List, Any, TypedDict, Annotated\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "# Tavily search tool from LangChain Community\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "def setup_logging(level: str = \"INFO\", log_file: str = None):\n",
    "    \"\"\"Setup logging configuration\"\"\"\n",
    "    \n",
    "    # Create custom formatter\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    \n",
    "    # Setup root logger\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.setLevel(getattr(logging, level.upper()))\n",
    "    \n",
    "    # Clear existing handlers\n",
    "    root_logger.handlers.clear()\n",
    "    \n",
    "    # Console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(formatter)\n",
    "    root_logger.addHandler(console_handler)\n",
    "    \n",
    "    # File handler (optional)\n",
    "    if log_file:\n",
    "        file_handler = logging.FileHandler(log_file, encoding='utf-8')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        root_logger.addHandler(file_handler)\n",
    "    \n",
    "    # Configure LangChain/LangGraph specific loggers\n",
    "    langchain_logger = logging.getLogger(\"langchain\")\n",
    "    langchain_logger.setLevel(getattr(logging, level.upper()))\n",
    "    \n",
    "    langgraph_logger = logging.getLogger(\"langgraph\")\n",
    "    langgraph_logger.setLevel(getattr(logging, level.upper()))\n",
    "    \n",
    "    # Create application logger\n",
    "    app_logger = logging.getLogger(\"research_agent\")\n",
    "    app_logger.setLevel(getattr(logging, level.upper()))\n",
    "    \n",
    "    return app_logger\n",
    "\n",
    "# Setup default logging\n",
    "logger = setup_logging(\"INFO\")\n",
    "logger.info(\"\\N{WHITE HEAVY CHECK MARK} All imports and logging setup successful!\")\n",
    "\n",
    "print(\"\\N{WHITE HEAVY CHECK MARK} All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b555cfe7",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09eb3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Configuration and LLM Setup\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # API Keys - set these in your .env file or environment\n",
    "        self.openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        self.anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\") \n",
    "        self.tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "        \n",
    "        # Default LLM settings\n",
    "        self.default_model = \"gpt-4o-mini\"  # or \"claude-3-haiku-20240307\"\n",
    "        self.default_temperature = 0.1\n",
    "        \n",
    "        # Validate API keys\n",
    "        if not self.tavily_api_key:\n",
    "            print(\"\\N{WARNING SIGN}  Warning: TAVILY_API_KEY not found. Web search will not work.\")\n",
    "        if not self.openai_api_key and not self.anthropic_api_key:\n",
    "            print(\"\\N{WARNING SIGN}  Warning: No LLM API keys found. Please set OPENAI_API_KEY or ANTHROPIC_API_KEY\")\n",
    "\n",
    "def get_llm(model_name: str = None, temperature: float = None):\n",
    "    \"\"\"Get configured LLM instance\"\"\"\n",
    "    config = Config()\n",
    "    model = model_name or config.default_model\n",
    "    temp = temperature if temperature is not None else config.default_temperature\n",
    "    \n",
    "    if model.startswith(\"gpt\"):\n",
    "        if not config.openai_api_key:\n",
    "            raise ValueError(\"OpenAI API key required for GPT models\")\n",
    "        return ChatOpenAI(\n",
    "            model=model,\n",
    "            temperature=temp,\n",
    "            api_key=config.openai_api_key\n",
    "        )\n",
    "    elif model.startswith(\"claude\"):\n",
    "        if not config.anthropic_api_key:\n",
    "            raise ValueError(\"Anthropic API key required for Claude models\")\n",
    "        return ChatAnthropic(\n",
    "            model=model,\n",
    "            temperature=temp,\n",
    "            api_key=config.anthropic_api_key\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model}\")\n",
    "\n",
    "# Test LLM setup\n",
    "try:\n",
    "    test_llm = get_llm()\n",
    "    print(f\"\\N{WHITE HEAVY CHECK MARK} LLM setup successful: {Config().default_model}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\N{CROSS MARK} LLM setup failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b6c279",
   "metadata": {},
   "source": [
    "# Define Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf7e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LangChain's built-in Tavily search tool\n",
    "def get_tavily_tool(max_results: int = 5):\n",
    "    \"\"\"Get configured Tavily search tool\"\"\"\n",
    "    return TavilySearchResults(\n",
    "        max_results=max_results,\n",
    "        search_depth=\"advanced\",\n",
    "        include_answer=True,\n",
    "        include_raw_content=True,\n",
    "        api_key=Config().tavily_api_key\n",
    "    )\n",
    "\n",
    "@tool \n",
    "def scrape_website(url: str) -> Dict[str, str]:\n",
    "    \"\"\"Scrape content from a website URL\"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "            script.decompose()\n",
    "            \n",
    "        # Get text content\n",
    "        text = soup.get_text()\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        # Limit content length\n",
    "        max_length = 5000\n",
    "        if len(text) > max_length:\n",
    "            text = text[:max_length] + \"...\"\n",
    "            \n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"title\": soup.title.string if soup.title else \"No title\",\n",
    "            \"content\": text,\n",
    "            \"status\": \"success\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"content\": \"\",\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b7933e",
   "metadata": {},
   "source": [
    "# Define Graph State Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e1c20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchState(TypedDict):\n",
    "    \"\"\"State schema for the research workflow\"\"\"\n",
    "    user_query: str\n",
    "    max_research_steps: int  # New field for step limit\n",
    "    plan: List[str]\n",
    "    current_step: int\n",
    "    research_data: List[Dict[str, Any]]\n",
    "    final_report: str\n",
    "    status: str\n",
    "    messages: List[Any]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184f0848",
   "metadata": {},
   "source": [
    "# Planner Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f495d984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Planner Agent\n",
    "def create_planner_agent(model_name: str = None, temperature: float = None):\n",
    "    \"\"\"Create the planner agent that handles both initial planning and revisions\"\"\"\n",
    "    llm = get_llm(model_name, temperature)\n",
    "    \n",
    "    planner_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are an expert research assistant. Create or revise a research plan to help gather comprehensive information on the given topic.\n",
    "        \n",
    "        For INITIAL planning:\n",
    "        - Break down research topic into NO MORE THAN {max_steps} sub-topics\n",
    "        - For each subtopic, add give a brief description of the actions to be taken by a researcher in the format: [subtopic]: [description]\n",
    "        - Each subtopic should be specific and actionable\n",
    "        - Subtopics must either be in a logical sequence or MECE (Mutually Exclusive, Collectively Exhaustive)\n",
    "        - Consider information sources needed for each subtopic\n",
    "\n",
    "        For REVISIONS:\n",
    "        - Review the conversation history for user feedback\n",
    "        - Address user concerns while staying within step limit\n",
    "        - Maintain plan quality and coherence\n",
    "\n",
    "        IMPORTANT: Create exactly {max_steps} steps or fewer. Quality over quantity.\n",
    "        Format as numbered list.\n",
    "\n",
    "        Query: {query}\n",
    "        Maximum steps: {max_steps}\n",
    "        \"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ])\n",
    "    \n",
    "    return planner_prompt | llm\n",
    "\n",
    "def planner_node(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"Planner node - creates or revises research plan based on message history\"\"\"\n",
    "    logger = logging.getLogger(\"research_agent.planner\")\n",
    "    \n",
    "    max_steps = state.get(\"max_research_steps\", 5)\n",
    "    \n",
    "    # Check if this is initial planning or revision\n",
    "    messages = state.get(\"messages\", [])\n",
    "    is_revision = any(msg.get(\"role\") == \"human\" and msg.get(\"content\") != \"Plan approved\" \n",
    "                    for msg in messages if isinstance(msg, dict))\n",
    "    \n",
    "    if is_revision:\n",
    "        logger.info(f\"\\N{CLOCKWISE DOWNWARDS AND UPWARDS OPEN CIRCLE ARROWS} Revising research plan based on user feedback\")\n",
    "    else:\n",
    "        logger.info(f\"\\N{DIRECT HIT} Creating initial research plan for: '{state['user_query']}'\")\n",
    "    \n",
    "    logger.info(f\"\\N{BAR CHART} Maximum research steps allowed: {max_steps}\")\n",
    "    \n",
    "    try:\n",
    "        planner = create_planner_agent()\n",
    "        \n",
    "        # Convert messages to proper format for the prompt\n",
    "        formatted_messages = []\n",
    "        for msg in messages:\n",
    "            if isinstance(msg, dict):\n",
    "                if msg.get(\"role\") == \"human\":\n",
    "                    formatted_messages.append(HumanMessage(content=msg[\"content\"]))\n",
    "                elif msg.get(\"role\") == \"ai\":\n",
    "                    formatted_messages.append(AIMessage(content=msg[\"content\"]))\n",
    "            else:\n",
    "                formatted_messages.append(msg)\n",
    "        \n",
    "        # Add initial human message if no messages exist\n",
    "        if not formatted_messages:\n",
    "            formatted_messages.append(HumanMessage(content=f\"Create a research plan for: {state['user_query']}\"))\n",
    "        \n",
    "        response = planner.invoke({\n",
    "            \"query\": state[\"user_query\"],\n",
    "            \"max_steps\": max_steps,\n",
    "            \"messages\": formatted_messages\n",
    "        })\n",
    "        \n",
    "        # Parse the plan\n",
    "        plan_text = response.content\n",
    "        plan_lines = [line.strip() for line in plan_text.split('\\n') \n",
    "                     if line.strip() and any(char.isdigit() for char in line[:3])]\n",
    "        \n",
    "        plan = []\n",
    "        for line in plan_lines:\n",
    "            cleaned = line.split('.', 1)[-1].strip() if '.' in line else line.strip()\n",
    "            if cleaned:\n",
    "                plan.append(cleaned)\n",
    "        \n",
    "        # Enforce step limit\n",
    "        if len(plan) > max_steps:\n",
    "            logger.warning(f\"\\N{WARNING SIGN} Plan exceeded limit, truncating to {max_steps} steps\")\n",
    "            plan = plan[:max_steps]\n",
    "        \n",
    "        action = \"Revised\" if is_revision else \"Generated\"\n",
    "        logger.info(f\"\\N{CLIPBOARD} {action} {len(plan)} research steps\")\n",
    "        \n",
    "        return {\n",
    "            **state,\n",
    "            \"plan\": plan,\n",
    "            \"current_step\": 0,\n",
    "            \"status\": \"planning_complete\",\n",
    "            \"messages\": state.get(\"messages\", []) + [response]\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"\\N{CROSS MARK} Planning failed: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ad390f",
   "metadata": {},
   "source": [
    "# Human Revision Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55e8ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6.5: Human Approval Node\n",
    "def human_approval_node(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"Human approval node - seeks user approval for the research plan\"\"\"\n",
    "    logger = logging.getLogger(\"research_agent.human_approval\")\n",
    "    \n",
    "    logger.info(\"\\N{BUST IN SILHOUETTE} Requesting human approval for research plan...\")\n",
    "    \n",
    "    # Display the plan to the user\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"\\N{CLIPBOARD} RESEARCH PLAN FOR APPROVAL\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Query: {state['user_query']}\")\n",
    "    print(f\"Max Steps: {state.get('max_research_steps', 5)}\")\n",
    "    print(\"\\nProposed Research Plan:\")\n",
    "    \n",
    "    for i, step in enumerate(state.get('plan', []), 1):\n",
    "        print(f\"  {i}. {step}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Please review the plan above.\")\n",
    "    print(\"- Type [APPROVE] to proceed with research\")\n",
    "    print(\"- Or provide feedback to modify the plan\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get user input\n",
    "    user_input = input(\"\\nYour response: \").strip()\n",
    "    \n",
    "    if user_input.upper() == \"[APPROVE]\":\n",
    "        logger.info(\"\\N{WHITE HEAVY CHECK MARK} Plan approved by user\")\n",
    "        return {\n",
    "            **state,\n",
    "            \"status\": \"plan_approved\",\n",
    "            \"messages\": state.get(\"messages\", []) + [{\"role\": \"human\", \"content\": \"Plan approved\"}]\n",
    "        }\n",
    "    else:\n",
    "        logger.info(f\"\\N{MEMO} User requested plan modification: {user_input}\")\n",
    "        return {\n",
    "            **state,\n",
    "            \"status\": \"plan_needs_revision\",\n",
    "            \"messages\": state.get(\"messages\", []) + [{\"role\": \"human\", \"content\": user_input}]\n",
    "        }\n",
    "\n",
    "def route_after_human_approval(state: ResearchState) -> str:\n",
    "    \"\"\"Route after human approval\"\"\"\n",
    "    if state.get(\"status\") == \"plan_approved\":\n",
    "        return \"start_research\"\n",
    "    elif state.get(\"status\") == \"plan_needs_revision\":\n",
    "        return \"revise_plan\"\n",
    "    else:\n",
    "        return \"human_approval\"  # Stay in approval loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2bdf4d",
   "metadata": {},
   "source": [
    "# Research Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2957539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Researcher Agent\n",
    "def create_researcher_agent(model_name: str = None, temperature: float = None):\n",
    "    \"\"\"Create the researcher agent with tools\"\"\"\n",
    "    llm = get_llm(model_name, temperature)\n",
    "    \n",
    "    # Use LangChain's built-in Tavily tool + custom scraping tool\n",
    "    tavily_tool = get_tavily_tool(max_results=10)\n",
    "    tools = [tavily_tool, scrape_website]\n",
    "    \n",
    "    # Use the correct 'prompt' parameter\n",
    "    system_prompt = \"\"\"You are a thorough researcher. Your job is to execute research steps using available tools.\n",
    "\n",
    "    Available tools:\n",
    "    - tavily_search_results_json: Search the web for information (returns JSON results)\n",
    "    - scrape_website: Extract content from specific URLs\n",
    "\n",
    "    Guidelines:\n",
    "    1. Use tavily_search_results_json to find relevant information sources\n",
    "    2. Use scrape_website to get detailed content from promising URLs found in search results\n",
    "    3. Gather comprehensive information for each research step\n",
    "    4. Focus on recent, credible sources\n",
    "    5. Extract key facts, statistics, and insights\n",
    "    6. Be thorough but efficient\"\"\"\n",
    "    \n",
    "    return create_react_agent(llm, tools, prompt=system_prompt)\n",
    "\n",
    "def researcher_node(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"Researcher node - executes current research step\"\"\"\n",
    "    logger = logging.getLogger(\"research_agent.researcher\")\n",
    "    \n",
    "    if state[\"current_step\"] >= len(state[\"plan\"]):\n",
    "        logger.info(\"\\N{WHITE HEAVY CHECK MARK} All research steps completed\")\n",
    "        return {**state, \"status\": \"research_complete\"}\n",
    "    \n",
    "    current_step = state[\"plan\"][state[\"current_step\"]]\n",
    "    step_num = state[\"current_step\"] + 1\n",
    "    \n",
    "    logger.info(f\"\\N{LEFT-POINTING MAGNIFYING GLASS} Executing research step {step_num}/{len(state['plan'])}: '{current_step}'\")\n",
    "    \n",
    "    try:\n",
    "        researcher = create_researcher_agent()\n",
    "        \n",
    "        # Prepare context from previous research\n",
    "        context = \"\"\n",
    "        if state.get(\"research_data\"):\n",
    "            context = f\"Previous research findings: {json.dumps(state['research_data'][-3:], indent=2)}\"\n",
    "            logger.debug(f\"Using context from {len(state['research_data'])} previous research steps\")\n",
    "        \n",
    "        # Execute research step\n",
    "        research_input = {\n",
    "            \"messages\": [HumanMessage(content=f\"Execute this research step: {current_step}\\n\\nContext: {context}\")]\n",
    "        }\n",
    "        \n",
    "        logger.debug(\"Invoking researcher agent...\")\n",
    "        result = researcher.invoke(research_input)\n",
    "        \n",
    "        # Extract research data from the result\n",
    "        research_data = state.get(\"research_data\", [])\n",
    "        new_finding = {\n",
    "            \"step\": step_num,\n",
    "            \"query\": current_step,\n",
    "            \"findings\": result[\"messages\"][-1].content,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        research_data.append(new_finding)\n",
    "        \n",
    "        logger.info(f\"\\N{WHITE HEAVY CHECK MARK} Research step {step_num} completed successfully\")\n",
    "        logger.debug(f\"Research findings length: {len(new_finding['findings'])} characters\")\n",
    "        \n",
    "        return {\n",
    "            **state,\n",
    "            \"research_data\": research_data,\n",
    "            \"current_step\": step_num,\n",
    "            \"status\": \"researching\",\n",
    "            \"messages\": state.get(\"messages\", []) + result[\"messages\"]\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"\\N{CROSS MARK} Research step {step_num} failed: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a90ab03",
   "metadata": {},
   "source": [
    "# Report Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cac10fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Report Writer Agent\n",
    "def create_report_writer_agent(model_name: str = None, temperature: float = None):\n",
    "    \"\"\"Create the report writer agent\"\"\"\n",
    "    llm = get_llm(model_name, temperature)\n",
    "    \n",
    "    report_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are an expert report writer. Your job is to synthesize research findings into a comprehensive, well-structured report.\n",
    "\n",
    "        Report Structure:\n",
    "        1. Executive Summary\n",
    "        2. Introduction\n",
    "        3. Key Findings (organized by themes)\n",
    "        4. Analysis and Insights\n",
    "        5. Conclusions\n",
    "        6. Sources and References\n",
    "\n",
    "        Guidelines:\n",
    "        - Write in clear, professional language\n",
    "        - Use headings and subheadings for organization\n",
    "        - Include specific data points and evidence\n",
    "        - Synthesize information rather than just listing facts\n",
    "        - Draw meaningful conclusions\n",
    "        - Cite sources where appropriate\n",
    "        - Aim for 1000-2000 words depending on complexity\n",
    "\n",
    "        Research Data: {research_data}\n",
    "        Original Query: {query}\"\"\"),\n",
    "        (\"human\", \"Please create a comprehensive report based on the research findings.\")\n",
    "    ])\n",
    "    \n",
    "    return report_prompt | llm\n",
    "\n",
    "def report_writer_node(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"Report writer node - creates final report\"\"\"\n",
    "    logger = logging.getLogger(\"research_agent.report_writer\")\n",
    "    \n",
    "    logger.info(\"\\N{MEMO} Starting report generation...\")\n",
    "    \n",
    "    try:\n",
    "        report_writer = create_report_writer_agent()\n",
    "        \n",
    "        # Prepare research data for the report\n",
    "        research_summary = \"\"\n",
    "        for i, data in enumerate(state.get(\"research_data\", []), 1):\n",
    "            research_summary += f\"\\n\\n## Research Step {i}: {data['query']}\\n{data['findings']}\"\n",
    "        \n",
    "        logger.debug(f\"Prepared research summary with {len(research_summary)} characters\")\n",
    "        \n",
    "        response = report_writer.invoke({\n",
    "            \"research_data\": research_summary,\n",
    "            \"query\": state[\"user_query\"]\n",
    "        })\n",
    "        \n",
    "        report_length = len(response.content)\n",
    "        logger.info(f\"\\N{WHITE HEAVY CHECK MARK} Report generated successfully ({report_length} characters)\")\n",
    "        \n",
    "        return {\n",
    "            **state,\n",
    "            \"final_report\": response.content,\n",
    "            \"status\": \"complete\",\n",
    "            \"messages\": state.get(\"messages\", []) + [response]\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"\\N{CROSS MARK} Report generation failed: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f187e241",
   "metadata": {},
   "source": [
    "# Decision Functions (routers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3780b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Decision Functions\n",
    "def should_continue_research(state: ResearchState) -> str:\n",
    "    \"\"\"Decide whether to continue research or move to report writing\"\"\"\n",
    "    if state[\"current_step\"] >= len(state.get(\"plan\", [])):\n",
    "        return \"write_report\"\n",
    "    else:\n",
    "        return \"continue_research\"\n",
    "\n",
    "# (CURRENTLY UNUSED) Function to start research\n",
    "def route_after_planning(state: ResearchState) -> str:\n",
    "    \"\"\"Route after planning is complete\"\"\"\n",
    "    return \"start_research\"\n",
    "\n",
    "def route_after_human_approval(state: ResearchState) -> str:\n",
    "    \"\"\"Route after human approval\"\"\"\n",
    "    if state.get(\"status\") == \"plan_approved\":\n",
    "        return \"start_research\"\n",
    "    elif state.get(\"status\") == \"plan_needs_revision\":\n",
    "        return \"revise_plan\"\n",
    "    else:\n",
    "        return \"human_approval\"  # Stay in approval loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704bd18d",
   "metadata": {},
   "source": [
    "# Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b07e12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Build the Graph (UPDATED)\n",
    "def create_research_workflow(model_name: str = None, temperature: float = None):\n",
    "    \"\"\"Create the complete research workflow graph\"\"\"\n",
    "    \n",
    "    # Initialize the graph\n",
    "    workflow = StateGraph(ResearchState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"planner\", planner_node)\n",
    "    workflow.add_node(\"human_approval\", human_approval_node)\n",
    "    workflow.add_node(\"researcher\", researcher_node)\n",
    "    workflow.add_node(\"report_writer\", report_writer_node)\n",
    "    \n",
    "    # Add edges\n",
    "    workflow.add_edge(START, \"planner\")\n",
    "    workflow.add_edge(\"planner\", \"human_approval\")\n",
    "    \n",
    "    # Add conditional edges\n",
    "    workflow.add_conditional_edges(\n",
    "        \"human_approval\",\n",
    "        route_after_human_approval,\n",
    "        {\n",
    "            \"start_research\": \"researcher\",\n",
    "            \"revise_plan\": \"planner\"  # Go back to planner for revision\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Add router to check if all subtopics have been researched\n",
    "    workflow.add_conditional_edges(\n",
    "        \"researcher\",\n",
    "        should_continue_research,\n",
    "        {\n",
    "            \"continue_research\": \"researcher\",\n",
    "            \"write_report\": \"report_writer\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    workflow.add_edge(\"report_writer\", END)\n",
    "    \n",
    "    # Add memory\n",
    "    memory = MemorySaver()\n",
    "    \n",
    "    # Compile the graph\n",
    "    app = workflow.compile(checkpointer=memory)\n",
    "    \n",
    "    return app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7a8ed3",
   "metadata": {},
   "source": [
    "# Main Execution Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0494946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Main Execution Function (Reverted to original, emojis replaced with \\N encoding)\n",
    "def run_research(query: str, model_name: str = None, temperature: float = None, max_research_steps: int = 5, log_level: str = \"INFO\", log_file: str = None):\n",
    "    \"\"\"Run the complete research workflow\"\"\"\n",
    "    \n",
    "    # Setup logging for this run\n",
    "    logger = setup_logging(log_level, log_file)\n",
    "    \n",
    "    logger.info(\"\\N{ROCKET}\" + \"=\"*60)\n",
    "    logger.info(f\"\\N{ROCKET} Starting research workflow\")\n",
    "    logger.info(f\"\\N{CLIPBOARD} Query: {query}\")\n",
    "    logger.info(f\"\\N{ROBOT FACE} Model: {model_name or Config().default_model}\")\n",
    "    logger.info(f\"\\N{THERMOMETER}  Temperature: {temperature if temperature is not None else Config().default_temperature}\")\n",
    "    logger.info(f\"\\N{BAR CHART} Max Research Steps: {max_research_steps}\")\n",
    "    logger.info(f\"\\N{BAR CHART} Log Level: {log_level}\")\n",
    "    if log_file:\n",
    "        logger.info(f\"\\N{PAGE FACING UP} Log File: {log_file}\")\n",
    "    logger.info(\"\\N{ROCKET}\" + \"=\"*60)\n",
    "    \n",
    "    # Create workflow\n",
    "    logger.debug(\"Creating workflow graph...\")\n",
    "    app = create_research_workflow(model_name, temperature)\n",
    "    \n",
    "    # Initial state\n",
    "    initial_state = {\n",
    "        \"user_query\": query,\n",
    "        \"max_research_steps\": max_research_steps,\n",
    "        \"plan\": [],\n",
    "        \"current_step\": 0,\n",
    "        \"research_data\": [],\n",
    "        \"final_report\": \"\",\n",
    "        \"status\": \"starting\",\n",
    "        \"messages\": []\n",
    "    }\n",
    "    \n",
    "    # Configuration for the run\n",
    "    config = {\"configurable\": {\"thread_id\": f\"research_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"}}\n",
    "    \n",
    "    try:\n",
    "        # Execute the workflow\n",
    "        logger.info(\"\\N{CLAPPER BOARD} Starting workflow execution...\")\n",
    "        final_state = None\n",
    "        step_count = 0\n",
    "        \n",
    "        for state in app.stream(initial_state, config):\n",
    "            step_count += 1\n",
    "            for node_name, node_state in state.items():\n",
    "                logger.info(f\"\\N{ROUND PUSHPIN} Step {step_count}: Executing node '{node_name}'\")\n",
    "                \n",
    "                if node_name == \"planner\" and node_state.get(\"plan\"):\n",
    "                    max_steps = node_state.get(\"max_research_steps\", 5)\n",
    "                    logger.info(f\"\\N{CLIPBOARD} Research Plan Created ({len(node_state['plan'])}/{max_steps} steps):\")\n",
    "                    for i, step in enumerate(node_state[\"plan\"], 1):\n",
    "                        logger.info(f\"   {i}. {step}\")\n",
    "                \n",
    "                elif node_name == \"researcher\":\n",
    "                    current_step = node_state.get(\"current_step\", 0)\n",
    "                    total_steps = len(node_state.get(\"plan\", []))\n",
    "                    if current_step <= total_steps:\n",
    "                        progress = f\"{current_step}/{total_steps}\"\n",
    "                        logger.info(f\"\\N{LEFT-POINTING MAGNIFYING GLASS} Research progress: {progress}\")\n",
    "                \n",
    "                elif node_name == \"report_writer\":\n",
    "                    logger.info(\"\\N{MEMO} Final report generated!\")\n",
    "                \n",
    "                final_state = node_state\n",
    "        \n",
    "        # Log completion summary\n",
    "        if final_state:\n",
    "            research_steps = len(final_state.get(\"research_data\", []))\n",
    "            report_length = len(final_state.get(\"final_report\", \"\"))\n",
    "            max_steps = final_state.get(\"max_research_steps\", \"N/A\")\n",
    "            \n",
    "            logger.info(\"\\N{WHITE HEAVY CHECK MARK}\" + \"=\"*60)\n",
    "            logger.info(\"\\N{WHITE HEAVY CHECK MARK} Research workflow completed successfully!\")\n",
    "            logger.info(f\"\\N{BAR CHART} Research steps executed: {research_steps}/{max_steps}\")\n",
    "            logger.info(f\"\\N{PAGE FACING UP} Final report length: {report_length} characters\")\n",
    "            logger.info(f\"\\N{STOPWATCH}  Workflow steps: {step_count}\")\n",
    "            logger.info(\"\\N{WHITE HEAVY CHECK MARK}\" + \"=\"*60)\n",
    "        \n",
    "        return final_state\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(\"\\N{CROSS MARK}\" + \"=\"*60)\n",
    "        logger.error(f\"\\N{CROSS MARK} Research workflow failed: {str(e)}\")\n",
    "        logger.error(\"\\N{CROSS MARK}\" + \"=\"*60)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3e6e2f",
   "metadata": {},
   "source": [
    "# Define Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5491ae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Example Usage and Testing\n",
    "def display_results(result_state):\n",
    "    \"\"\"Display the research results in a formatted way\"\"\"\n",
    "    if not result_state:\n",
    "        print(\"\\N{CROSS MARK} No results to display\")\n",
    "        return\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\N{DIRECT HIT} RESEARCH RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    print(f\"\\n\\N{MEMO} Original Query: {result_state.get('user_query', 'N/A')}\")\n",
    "\n",
    "    if result_state.get('plan'):\n",
    "        print(f\"\\n\\N{CLIPBOARD} Research Plan:\")\n",
    "        for i, step in enumerate(result_state['plan'], 1):\n",
    "            print(f\"   {i}. {step}\")\n",
    "\n",
    "    if result_state.get('research_data'):\n",
    "        print(f\"\\n\\N{LEFT-POINTING MAGNIFYING GLASS} Research Steps Completed: {len(result_state['research_data'])}\")\n",
    "\n",
    "    if result_state.get('final_report'):\n",
    "        print(f\"\\n\\N{PAGE FACING UP} Final Report:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(result_state['final_report'])\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Test the system with a sample query\n",
    "print(\"\\N{TEST TUBE} Testing the system...\")\n",
    "print(\"You can now run research queries using the run_research() function!\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"result = run_research('What are the latest developments in AI safety research?')\")\n",
    "print(\"display_results(result)\")\n",
    "\n",
    "# Ready to use!\n",
    "print(\"\\n\\N{WHITE HEAVY CHECK MARK} Multi-Agent Web Research System is ready!\")\n",
    "print(\"\\nTo start researching:\")\n",
    "print(\"result = run_research('your query here', max_research_steps=3)\")\n",
    "print(\"display_results(result)\")\n",
    "\n",
    "print(\"\\n\\N{WRENCH} Research Step Limit Feature:\")\n",
    "print(\"- Default: 5 steps\")\n",
    "print(\"- Range: 1-10 steps\")\n",
    "print(\"- Usage: run_research('query', max_research_steps=3)\")\n",
    "\n",
    "print(\"\\n\\N{BAR CHART} Available Parameters:\")\n",
    "print(\"- query: str (required)\")\n",
    "print(\"- model_name: str (optional, e.g., 'gpt-4o-mini')\")\n",
    "print(\"- temperature: float (optional, 0.0-2.0)\")\n",
    "print(\"- max_research_steps: int (optional, 1-10, default=5)\")\n",
    "print(\"- log_level: str (optional, 'INFO'/'DEBUG'/'WARNING')\")\n",
    "print(\"- log_file: str (optional, path to save logs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ef5f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Uncomment the lines below to run a test\n",
    "test_result = run_research(\n",
    "    \"What are the current trends in renewable energy adoption?\",\n",
    "    max_research_steps=5,\n",
    "    log_level=\"DEBUG\",\n",
    "    log_file=\"research_seq.log\"\n",
    "    )\n",
    "\n",
    "display_results(test_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
